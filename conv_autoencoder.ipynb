{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\python310\\lib\\site-packages (0.12.16)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\python310\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\python310\\lib\\site-packages (from wandb) (1.5.11)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\python310\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in c:\\python310\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\python310\\lib\\site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\python310\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\python310\\lib\\site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\python310\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\python310\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\python310\\lib\\site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\python310\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\python310\\lib\\site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: setproctitle in c:\\python310\\lib\\site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\python310\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: graphviz in c:\\python310\\lib\\site-packages (0.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchviz in c:\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (from torchviz) (1.11.0+cu113)\n",
      "Requirement already satisfied: graphviz in c:\\python310\\lib\\site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: typing-extensions in c:\\python310\\lib\\site-packages (from torch->torchviz) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtetetoutou\u001b[0m (\u001b[33msimclr-doctoral-research\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%pip install graphviz\n",
    "%pip install torchviz\n",
    "import wandb\n",
    "wandb.login()#doesnt detect WANDB_NOTEBOOK_NAME on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\theob\\simclr_doctoral_research-1\\wandb\\run-20220506_000556-jmk8j0qg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simclr-doctoral-research/test-project/runs/jmk8j0qg\" target=\"_blank\">forgotten-shuttle-3</a></strong> to <a href=\"https://wandb.ai/simclr-doctoral-research/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/simclr-doctoral-research/test-project/runs/jmk8j0qg?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1f11964cdc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"test-project\", entity=\"simclr-doctoral-research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.9249\n",
      "epoch [2/100], loss:0.9080\n",
      "epoch [3/100], loss:0.8764\n",
      "epoch [4/100], loss:0.8688\n",
      "epoch [5/100], loss:0.8692\n",
      "epoch [6/100], loss:0.8647\n",
      "epoch [7/100], loss:0.8669\n",
      "epoch [8/100], loss:0.8589\n",
      "epoch [9/100], loss:0.8537\n",
      "epoch [10/100], loss:0.8506\n",
      "epoch [11/100], loss:0.8512\n",
      "epoch [12/100], loss:0.8630\n",
      "epoch [13/100], loss:0.8604\n",
      "epoch [14/100], loss:0.8548\n",
      "epoch [15/100], loss:0.8526\n",
      "epoch [16/100], loss:0.8560\n",
      "epoch [17/100], loss:0.8553\n",
      "epoch [18/100], loss:0.8609\n",
      "epoch [19/100], loss:0.8504\n",
      "epoch [20/100], loss:0.8485\n",
      "epoch [21/100], loss:0.8497\n",
      "epoch [22/100], loss:0.8504\n",
      "epoch [23/100], loss:0.8408\n",
      "epoch [24/100], loss:0.8516\n",
      "epoch [25/100], loss:0.8501\n",
      "epoch [26/100], loss:0.8545\n",
      "epoch [27/100], loss:0.8460\n",
      "epoch [28/100], loss:0.8506\n",
      "epoch [29/100], loss:0.8473\n",
      "epoch [30/100], loss:0.8474\n",
      "epoch [31/100], loss:0.8522\n",
      "epoch [32/100], loss:0.8490\n",
      "epoch [33/100], loss:0.8517\n",
      "epoch [34/100], loss:0.8526\n",
      "epoch [35/100], loss:0.8660\n",
      "epoch [36/100], loss:0.8507\n",
      "epoch [37/100], loss:0.8491\n",
      "epoch [38/100], loss:0.8470\n",
      "epoch [39/100], loss:0.8525\n",
      "epoch [40/100], loss:0.8427\n",
      "epoch [41/100], loss:0.8541\n",
      "epoch [42/100], loss:0.8473\n",
      "epoch [43/100], loss:0.8592\n",
      "epoch [44/100], loss:0.8526\n",
      "epoch [45/100], loss:0.8588\n",
      "epoch [46/100], loss:0.8511\n",
      "epoch [47/100], loss:0.8489\n",
      "epoch [48/100], loss:0.8534\n",
      "epoch [49/100], loss:0.8552\n",
      "epoch [50/100], loss:0.8457\n",
      "epoch [51/100], loss:0.8575\n",
      "epoch [52/100], loss:0.8439\n",
      "epoch [53/100], loss:0.8527\n",
      "epoch [54/100], loss:0.8495\n",
      "epoch [55/100], loss:0.8488\n",
      "epoch [56/100], loss:0.8485\n",
      "epoch [57/100], loss:0.8515\n",
      "epoch [58/100], loss:0.8473\n",
      "epoch [59/100], loss:0.8553\n",
      "epoch [60/100], loss:0.8481\n",
      "epoch [61/100], loss:0.8500\n",
      "epoch [62/100], loss:0.8480\n",
      "epoch [63/100], loss:0.8446\n",
      "epoch [64/100], loss:0.8506\n",
      "epoch [65/100], loss:0.8459\n",
      "epoch [66/100], loss:0.8579\n",
      "epoch [67/100], loss:0.8585\n",
      "epoch [68/100], loss:0.8524\n",
      "epoch [69/100], loss:0.8597\n",
      "epoch [70/100], loss:0.8522\n",
      "epoch [71/100], loss:0.8476\n",
      "epoch [72/100], loss:0.8541\n",
      "epoch [73/100], loss:0.8484\n",
      "epoch [74/100], loss:0.8499\n",
      "epoch [75/100], loss:0.8501\n",
      "epoch [76/100], loss:0.8497\n",
      "epoch [77/100], loss:0.8561\n",
      "epoch [78/100], loss:0.8471\n",
      "epoch [79/100], loss:0.8527\n",
      "epoch [80/100], loss:0.8429\n",
      "epoch [81/100], loss:0.8436\n",
      "epoch [82/100], loss:0.8421\n",
      "epoch [83/100], loss:0.8513\n",
      "epoch [84/100], loss:0.8575\n",
      "epoch [85/100], loss:0.8436\n",
      "epoch [86/100], loss:0.8521\n",
      "epoch [87/100], loss:0.8599\n",
      "epoch [88/100], loss:0.8546\n",
      "epoch [89/100], loss:0.8481\n",
      "epoch [90/100], loss:0.8493\n",
      "epoch [91/100], loss:0.8459\n",
      "epoch [92/100], loss:0.8455\n",
      "epoch [93/100], loss:0.8471\n",
      "epoch [94/100], loss:0.8488\n",
      "epoch [95/100], loss:0.8464\n",
      "epoch [96/100], loss:0.8422\n",
      "epoch [97/100], loss:0.8592\n",
      "epoch [98/100], loss:0.8505\n",
      "epoch [99/100], loss:0.8499\n",
      "epoch [100/100], loss:0.8510\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        #img = img.view(img.size(0), -1)//makes the image 3D i think\n",
    "        img = Variable(img).to(device)\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.item()))\n",
    "    #if epoch % 10 == 0:\n",
    "        #pic = to_img(output.cpu().data)\n",
    "        #save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "    wandb.log({\"loss\": loss})\n",
    "\n",
    "    wandb.watch(model)\n",
    "    # print('epoch [{}/{}], loss:{:.4f}'\n",
    "    # #       .format(epoch + 1, num_epochs, loss.data[0]))\n",
    "    # if epoch % 10 == 0:\n",
    "    #     pic = to_img(output.cpu().data)\n",
    "    #     save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torchviz.png'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "y = model(img)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"torchviz\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
