{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\python310\\lib\\site-packages (0.12.16)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\python310\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\python310\\lib\\site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\python310\\lib\\site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: setproctitle in c:\\python310\\lib\\site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\python310\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\python310\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\python310\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in c:\\python310\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\python310\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\python310\\lib\\site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: pathtools in c:\\python310\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\python310\\lib\\site-packages (from wandb) (1.5.11)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\python310\\lib\\site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\python310\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\python310\\lib\\site-packages (0.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in c:\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (from torchviz) (1.11.0+cu113)\n",
      "Requirement already satisfied: graphviz in c:\\python310\\lib\\site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: typing-extensions in c:\\python310\\lib\\site-packages (from torch->torchviz) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtetetoutou\u001b[0m (\u001b[33msimclr-doctoral-research\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%pip install graphviz\n",
    "%pip install torchviz\n",
    "import wandb\n",
    "wandb.login()#doesnt detect WANDB_NOTEBOOK_NAME on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\theob\\simclr_doctoral_research-1\\wandb\\run-20220518_002409-3qgex19b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simclr-doctoral-research/test-project/runs/3qgex19b\" target=\"_blank\">autumn-forest-45</a></strong> to <a href=\"https://wandb.ai/simclr-doctoral-research/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/simclr-doctoral-research/test-project/runs/3qgex19b?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x15b94b0de40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"test-project\", entity=\"simclr-doctoral-research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 3, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([96, 1, 28, 28])) that is different to the input size (torch.Size([96, 3, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss:0.9282\n",
      "epoch [2/10], loss:0.8771\n",
      "epoch [3/10], loss:0.8668\n",
      "epoch [4/10], loss:0.8703\n",
      "epoch [5/10], loss:0.8594\n",
      "epoch [6/10], loss:0.8577\n",
      "epoch [7/10], loss:0.8665\n",
      "epoch [8/10], loss:0.8618\n",
      "epoch [9/10], loss:0.8709\n",
      "epoch [10/10], loss:0.8536\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        #img = img.view(img.size(0), -1)//makes the image 3D i think\n",
    "        img = Variable(img).to(device)\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.item()))\n",
    "    #if epoch % 10 == 0:\n",
    "        #pic = to_img(output.cpu().data)\n",
    "        #save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "    wandb.log({\"loss\": loss})\n",
    "\n",
    "    wandb.watch(model)\n",
    "    # print('epoch [{}/{}], loss:{:.4f}'\n",
    "    # #       .format(epoch + 1, num_epochs, loss.data[0]))\n",
    "    # if epoch % 10 == 0:\n",
    "    #     pic = to_img(output.cpu().data)\n",
    "    #     save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torchviz.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "y = model(img)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"torchviz_conv_autoencoder\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theob\\simclr_doctoral_research-1\\conv_autoencoder.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=15'>16</a>\u001b[0m         plt\u001b[39m.\u001b[39mimshow(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=16'>17</a>\u001b[0m         plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=18'>19</a>\u001b[0m show_the_model(model,img)\n",
      "\u001b[1;32mc:\\Users\\theob\\simclr_doctoral_research-1\\conv_autoencoder.ipynb Cell 8'\u001b[0m in \u001b[0;36mshow_the_model\u001b[1;34m(model, image_batch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_the_model\u001b[39m(model,image_batch):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=6'>7</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=7'>8</a>\u001b[0m         output \u001b[39m=\u001b[39m model(image_batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=8'>9</a>\u001b[0m         \u001b[39m#pic = to_img(output.cpu().data)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=9'>10</a>\u001b[0m         \u001b[39m#if the folder doesnt exist, create it\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000008?line=10'>11</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m'\u001b[39m\u001b[39m./conv_entoencoder\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\theob\\simclr_doctoral_research-1\\conv_autoencoder.ipynb Cell 5'\u001b[0m in \u001b[0;36mautoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000004?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000004?line=46'>47</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000004?line=47'>48</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research-1/conv_autoencoder.ipynb#ch0000004?line=48'>49</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "#show the resulting image of the model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "\n",
    "def show_the_model(model,image_batch):\n",
    "    with torch.no_grad():\n",
    "        output = model(image_batch)\n",
    "        #pic = to_img(output.cpu().data)\n",
    "        #if the folder doesnt exist, create it\n",
    "        if not os.path.exists('./conv_entoencoder'):\n",
    "            os.mkdir('./conv_entoencoder')\n",
    "\n",
    "        save_image(output.cpu().data, './conv_entoencoder/image.png')\n",
    "        img = mpimg.imread('./conv_entoencoder/image.png')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "show_the_model(model,img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the directoruy does not exist, create it\n",
    "if not os.path.exists('./saved_models'):\n",
    "    os.makedirs('./saved_models')\n",
    "torch.save(model.state_dict(), './saved_models/conv_autoencoder.pth')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
