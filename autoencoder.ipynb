{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb\n",
    "%pip install graphviz\n",
    "%pip install torchviz\n",
    "import wandb\n",
    "wandb.login()#doesnt detect WANDB_NOTEBOOK_NAME on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"test-project\", entity=\"simclr-doctoral-research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset_train = MNIST('./data', transform=img_transform, download=True,train = True)\n",
    "dataset_test = MNIST('./data', transform=img_transform, download=True,train = False)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256,28 * 28), \n",
    "            nn.Tanh())\n",
    "    def forward(self, x,only_encode=False):\n",
    "        if only_encode:\n",
    "            return self.encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "auto_model = AutoEncoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, auto_model.parameters()), lr=learning_rate, weight_decay=1e-5)#this line is needed\n",
    "#to freeze gradients\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).to(device)\n",
    "        # ===================forward=====================\n",
    "        output = auto_model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.item()))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "    wandb.log({\"loss\": loss})\n",
    "\n",
    "    wandb.watch(auto_model)\n",
    "    #print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data[0]))\n",
    "\n",
    "pic = to_img(output.cpu().data)\n",
    "save_image(pic, './mlp_img/image_final.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test of the model\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for data in test_data:\n",
    "    img, _ = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    output = auto_model(img)\n",
    "    pic = to_img(output.cpu().data)\n",
    "    save_image(pic, './mlp_img/image_test.png')\n",
    "#accuracy of the autoencoder comparing input and output\n",
    "loss_sum = 0\n",
    "for data in test_data:\n",
    "    img, _ = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    output = auto_model(img)\n",
    "    loss = criterion(output, img)\n",
    "    loss_sum += loss.item()\n",
    "\n",
    "print(loss_sum/len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "y = auto_model(img)\n",
    "make_dot(y, params=dict(list(auto_model.named_parameters()))).render(\"torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the encoder is going to be frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the encoder\n",
    "# print(model.state_dict())\n",
    "\n",
    "auto_model.encoder[0].weight.requires_grad = False\n",
    "auto_model.encoder[0].bias.requires_grad = False\n",
    "auto_model.encoder[2].weight.requires_grad = False\n",
    "auto_model.encoder[2].bias.requires_grad = False\n",
    "\n",
    "\n",
    "for name, param in auto_model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# if param.requires_grad:print(name)\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "#     param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the encoder\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "torch.save(auto_model.state_dict(), './saved_models/autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_classifier import LinearClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load the encoder\n",
    "# model.load_state_dict(torch.load('./saved_models/autoencoder.pth'))\n",
    "# #add a mlp to the encoder\n",
    "# model.add_module('linear_classifier', linear_classifier())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined model\n",
    "class JoinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JoinedModel, self).__init__()\n",
    "        self.encoder = auto_model.encoder\n",
    "        self.classifier = LinearClassifier()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "joined_model = JoinedModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_model.encoder[0].weight.requires_grad = False\n",
    "joined_model.encoder[0].bias.requires_grad = False\n",
    "joined_model.encoder[2].weight.requires_grad = False\n",
    "joined_model.encoder[2].bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify weight are frozen\n",
    "\n",
    "for name, param in joined_model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "print(joined_model.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model on the labeled data\n",
    "# model = model.to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, joined_model.parameters()), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).to(device)\n",
    "        label = Variable(label).to(device)\n",
    "        # ===================forward=====================\n",
    "        output = joined_model(img)#This should only use the encoder\n",
    "        loss = criterion(output, label)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "            .format(epoch + 1, num_epochs, loss.item()))\n",
    "    wandb.log({\"loss\": loss})\n",
    "    wandb.watch(joined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "loss_sum = 0\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "for data in test_data:\n",
    "    img, label = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    label = Variable(label).to(device)\n",
    "    output = joined_model(img)\n",
    "    loss = criterion(output, label)\n",
    "    loss_sum += loss.item()\n",
    "\n",
    "\n",
    "print(loss_sum/len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from fastai.vision.all import show_image\n",
    "#show results of the model\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "for data in test_data:\n",
    "    img, label = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    #label = Variable(label).to(device)\n",
    "    output = joined_model(img)\n",
    "    pic = to_img(img.cpu().data)\n",
    "    # save_image(pic, './mlp_img/image_test.png')\n",
    "    #show predicted label for the image\n",
    "    #print(output.argmax(dim=1))\n",
    "    #show the true label for the image\n",
    "    #print(label)\n",
    "    #show the image\n",
    "    plt.imshow(np.transpose(img.view(img.size(0), 1, 28, 28).cpu().data[0], (1,2, 0)))\n",
    "    #show the label corresponding to the image\n",
    "    plt.title(str(label[0])+str(output.argmax(dim=1)[0]))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model:\n",
    "joined_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_data:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).to(device)\n",
    "        label = Variable(label).to(device)\n",
    "        output = joined_model(img)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(\"accuracy: \", correct/total)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the joined_model\n",
    "torch.save(joined_model.state_dict(), './saved_models/joined_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
