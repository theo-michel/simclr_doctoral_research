{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\python310\\lib\\site-packages (0.12.16)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\python310\\lib\\site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\python310\\lib\\site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\python310\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\python310\\lib\\site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\python310\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: setproctitle in c:\\python310\\lib\\site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\python310\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\python310\\lib\\site-packages (from wandb) (1.5.11)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\python310\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\python310\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\python310\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\python310\\lib\\site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: PyYAML in c:\\python310\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\python310\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: graphviz in c:\\python310\\lib\\site-packages (0.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in c:\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (from torchviz) (1.11.0+cu113)\n",
      "Requirement already satisfied: graphviz in c:\\python310\\lib\\site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: typing-extensions in c:\\python310\\lib\\site-packages (from torch->torchviz) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%pip install graphviz\n",
    "%pip install torchviz\n",
    "import wandb\n",
    "wandb.login()#doesnt detect WANDB_NOTEBOOK_NAME on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1v8bx883) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1cdbf094c54ad389fbbe8754eb0e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.03818</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vibrant-fog-16</strong>: <a href=\"https://wandb.ai/simclr-doctoral-research/test-project/runs/1v8bx883\" target=\"_blank\">https://wandb.ai/simclr-doctoral-research/test-project/runs/1v8bx883</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220510_021053-1v8bx883\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1v8bx883). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\theob\\simclr_doctoral_research\\wandb\\run-20220510_021213-2s8qzwwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simclr-doctoral-research/test-project/runs/2s8qzwwa\" target=\"_blank\">polar-salad-17</a></strong> to <a href=\"https://wandb.ai/simclr-doctoral-research/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/simclr-doctoral-research/test-project/runs/2s8qzwwa?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x21685407220>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"test-project\", entity=\"simclr-doctoral-research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset_train = MNIST('./data', transform=img_transform, download=True,train = True)\n",
    "dataset_test = MNIST('./data', transform=img_transform, download=True,train = False)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256,28 * 28), \n",
    "            nn.Tanh())\n",
    "    def forward(self, x,only_encode=False):\n",
    "        if only_encode:\n",
    "            return self.encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch [1/10], loss:0.0942\n",
      "epoch [2/10], loss:0.0605\n",
      "epoch [3/10], loss:0.0395\n",
      "epoch [4/10], loss:0.0335\n",
      "epoch [5/10], loss:0.0304\n",
      "epoch [6/10], loss:0.0250\n",
      "epoch [7/10], loss:0.0277\n",
      "epoch [8/10], loss:0.0257\n",
      "epoch [9/10], loss:0.0209\n",
      "epoch [10/10], loss:0.0216\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=1e-5)#this line is needed\n",
    "#to freeze gradients\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).to(device)\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.item()))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "    wandb.log({\"loss\": loss})\n",
    "\n",
    "    wandb.watch(model)\n",
    "    #print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data[0]))\n",
    "\n",
    "pic = to_img(output.cpu().data)\n",
    "save_image(pic, './mlp_img/image_final.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020153347589075565\n"
     ]
    }
   ],
   "source": [
    "#test of the model\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for data in test_data:\n",
    "    img, _ = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    output = model(img)\n",
    "    pic = to_img(output.cpu().data)\n",
    "    save_image(pic, './mlp_img/image_test.png')\n",
    "#accuracy of the autoencoder comparing input and output\n",
    "loss_sum = 0\n",
    "for data in test_data:\n",
    "    img, _ = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    output = model(img)\n",
    "    loss = criterion(output, img)\n",
    "    loss_sum += loss.item()\n",
    "\n",
    "print(loss_sum/len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torchviz.png'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "y = model(img)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the encoder is going to be frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.weight False\n",
      "encoder.0.bias False\n",
      "encoder.2.weight False\n",
      "encoder.2.bias False\n",
      "decoder.0.weight True\n",
      "decoder.0.bias True\n",
      "decoder.2.weight True\n",
      "decoder.2.bias True\n"
     ]
    }
   ],
   "source": [
    "#freeze the encoder\n",
    "# print(model.state_dict())\n",
    "\n",
    "model.encoder[0].weight.requires_grad = False\n",
    "model.encoder[0].bias.requires_grad = False\n",
    "model.encoder[2].weight.requires_grad = False\n",
    "model.encoder[2].bias.requires_grad = False\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# if param.requires_grad:print(name)\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "#     param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the encoder\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "torch.save(model.state_dict(), './saved_models/autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(linear_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the encoder\n",
    "model.load_state_dict(torch.load('./saved_models/autoencoder.pth'))\n",
    "#add a mlp to the encoder\n",
    "model.add_module('linear_classifier', linear_classifier())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined model\n",
    "class joined_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(joined_model, self).__init__()\n",
    "        self.encoder = autoencoder.encoder\n",
    "        self.classifier = linear_classifier()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "joined_model = joined_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.weight False\n",
      "encoder.0.bias False\n",
      "encoder.2.weight False\n",
      "encoder.2.bias False\n",
      "classifier.fc1.weight True\n",
      "classifier.fc1.bias True\n",
      "classifier.fc2.weight True\n",
      "classifier.fc2.bias True\n",
      "<bound method Module.parameters of autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=784, bias=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (linear_classifier): linear_classifier(\n",
      "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "#verify weight are frozen\n",
    "\n",
    "for name, param in joined_model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "print(model.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss:0.2708\n",
      "epoch [2/10], loss:0.3691\n",
      "epoch [3/10], loss:0.4757\n",
      "epoch [4/10], loss:0.4167\n",
      "epoch [5/10], loss:0.3336\n",
      "epoch [6/10], loss:0.3332\n",
      "epoch [7/10], loss:0.4741\n",
      "epoch [8/10], loss:0.2894\n",
      "epoch [9/10], loss:0.2555\n",
      "epoch [10/10], loss:0.2197\n"
     ]
    }
   ],
   "source": [
    "#train the model on the labeled data\n",
    "# model = model.to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, joined_model.parameters()), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).to(device)\n",
    "        label = Variable(label).to(device)\n",
    "        # ===================forward=====================\n",
    "        output = joined_model(img)#This should only use the encoder\n",
    "        loss = criterion(output, label)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "            .format(epoch + 1, num_epochs, loss.item()))\n",
    "    wandb.log({\"loss\": loss})\n",
    "    wandb.watch(joined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30486280769109725\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "loss_sum = 0\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "for data in test_data:\n",
    "    img, label = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    label = Variable(label).to(device)\n",
    "    output = joined_model(img)\n",
    "    loss = criterion(output, label)\n",
    "    loss_sum += loss.item()\n",
    "print(loss_sum/len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 6, 5, 0, 1, 4, 9, 2, 1, 2, 7, 3, 9, 8, 5, 4, 3, 3, 1, 2, 3, 0, 3,\n",
      "        3, 3, 7, 4, 5, 9, 1, 8, 7, 1, 6, 4, 4, 6, 9, 5, 9, 7, 1, 0, 7, 9, 3, 3,\n",
      "        8, 2, 8, 0, 1, 4, 9, 0, 4, 2, 7, 4, 6, 4, 6, 7, 8, 3, 1, 4, 8, 0, 2, 1,\n",
      "        8, 0, 8, 3, 2, 8, 7, 5, 0, 6, 6, 5, 1, 5, 2, 7, 5, 5, 4, 5, 7, 6, 2, 8,\n",
      "        3, 0, 4, 9, 5, 0, 2, 8, 6, 6, 8, 3, 8, 8, 8, 0, 0, 7, 2, 6, 2, 4, 4, 7,\n",
      "        5, 4, 1, 0, 8, 2, 7, 6, 8, 1, 3, 6, 9, 6, 0, 2, 4, 9, 7, 5, 1, 8, 0, 7,\n",
      "        1, 1, 6, 4, 4, 1, 2, 5, 7, 4, 1, 7, 2, 2, 2, 9, 3, 5, 3, 1, 7, 0, 0, 0,\n",
      "        7, 8, 2, 7, 1, 9, 5, 3, 5, 2, 1, 6, 3, 2, 3, 7, 3, 1, 6, 0, 0, 5, 7, 1,\n",
      "        8, 8, 5, 4, 8, 5, 7, 7, 9, 9, 2, 3, 3, 9, 3, 8, 3, 7, 3, 6, 3, 6, 0, 7,\n",
      "        1, 5, 0, 8, 8, 3, 8, 5, 1, 1, 8, 1, 4, 3, 2, 3, 7, 9, 1, 3, 0, 7, 4, 1,\n",
      "        5, 3, 3, 1, 6, 2, 3, 5, 8, 6, 6, 1, 9, 8, 4, 2, 8, 1, 7, 7, 9, 5, 3, 4,\n",
      "        7, 1, 8, 6, 0, 9, 6, 1, 8, 6, 2, 1, 9, 3, 2, 2, 3, 3, 8, 7, 4, 2, 7, 1,\n",
      "        8, 2, 7, 3, 1, 1, 5, 3, 2, 1, 0, 2, 1, 4, 6, 8, 7, 2, 0, 6, 0, 6, 2, 6,\n",
      "        0, 9, 7, 4, 8, 5, 2, 7, 5, 9, 5, 3, 5, 5, 4, 3, 3, 0, 4, 6, 1, 3, 8, 1,\n",
      "        2, 0, 6, 6, 8, 3, 6, 5, 8, 7, 9, 3, 8, 9, 7, 3, 1, 1, 0, 1, 4, 8, 1, 1,\n",
      "        3, 1, 5, 9, 0, 2, 6, 5, 6, 7, 6, 1, 2, 3, 7, 8, 1, 7, 9, 7, 3, 5, 0, 2,\n",
      "        9, 2, 6, 5, 5, 9, 6, 2, 7, 2, 4, 9, 8, 1, 7, 8, 5, 9, 6, 0, 3, 0, 5, 2,\n",
      "        9, 1, 2, 6, 7, 4, 9, 1, 1, 4, 1, 1, 3, 5, 9, 6, 4, 4, 0, 2, 2, 3, 3, 7,\n",
      "        1, 8, 4, 8, 1, 6, 6, 1, 9, 5, 7, 9, 7, 8, 4, 1, 9, 7, 3, 4, 9, 0, 3, 0,\n",
      "        2, 7, 8, 7, 4, 9, 9, 5, 1, 8, 4, 1, 3, 8, 0, 8, 4, 4, 2, 2, 2, 7, 9, 7,\n",
      "        3, 5, 3, 6, 9, 6, 4, 4, 6, 5, 9, 1, 5, 8, 2, 7, 4, 1, 5, 9, 7, 8, 0, 3,\n",
      "        9, 0, 2, 9, 4, 4, 8, 1], device='cuda:0')\n",
      "tensor([5, 0, 6, 5, 5, 1, 4, 9, 2, 1, 2, 8, 3, 9, 8, 5, 4, 3, 3, 1, 2, 5, 0, 3,\n",
      "        3, 3, 7, 4, 8, 9, 1, 8, 7, 1, 6, 4, 9, 6, 9, 5, 9, 8, 1, 0, 7, 9, 3, 3,\n",
      "        8, 2, 8, 0, 1, 4, 9, 0, 4, 1, 7, 4, 4, 4, 6, 7, 8, 5, 1, 9, 8, 0, 2, 1,\n",
      "        8, 8, 5, 3, 2, 8, 7, 5, 5, 6, 6, 5, 1, 5, 2, 7, 5, 8, 4, 5, 7, 5, 2, 8,\n",
      "        3, 0, 4, 9, 5, 0, 2, 8, 6, 6, 8, 3, 8, 8, 8, 0, 0, 7, 2, 6, 2, 4, 4, 7,\n",
      "        5, 4, 1, 0, 8, 2, 7, 6, 8, 1, 5, 6, 9, 6, 0, 2, 4, 4, 7, 5, 1, 8, 0, 7,\n",
      "        1, 1, 6, 4, 4, 1, 2, 5, 7, 4, 1, 7, 2, 2, 2, 9, 3, 5, 3, 1, 7, 0, 0, 0,\n",
      "        7, 8, 2, 7, 1, 9, 5, 3, 5, 7, 1, 6, 3, 2, 3, 7, 3, 1, 6, 0, 0, 5, 7, 1,\n",
      "        8, 8, 5, 4, 8, 5, 7, 7, 9, 9, 2, 3, 3, 9, 3, 8, 3, 7, 3, 6, 2, 6, 0, 7,\n",
      "        1, 5, 0, 8, 8, 3, 8, 5, 1, 1, 8, 1, 4, 3, 2, 3, 7, 9, 1, 3, 0, 7, 4, 1,\n",
      "        5, 3, 3, 1, 6, 2, 3, 5, 8, 6, 0, 1, 9, 8, 4, 2, 8, 1, 7, 8, 9, 5, 3, 4,\n",
      "        7, 1, 2, 6, 0, 9, 6, 1, 8, 6, 2, 1, 9, 3, 1, 2, 3, 3, 4, 7, 4, 2, 7, 1,\n",
      "        3, 2, 7, 5, 1, 1, 5, 3, 2, 1, 0, 2, 1, 4, 5, 8, 7, 2, 0, 6, 0, 6, 2, 0,\n",
      "        0, 9, 7, 4, 8, 5, 2, 7, 5, 4, 5, 3, 5, 5, 4, 3, 3, 0, 4, 6, 1, 3, 8, 1,\n",
      "        2, 0, 0, 6, 9, 5, 6, 5, 8, 9, 9, 3, 8, 9, 7, 8, 1, 2, 0, 1, 4, 8, 1, 1,\n",
      "        3, 1, 5, 9, 0, 2, 6, 5, 6, 7, 6, 1, 2, 3, 2, 8, 1, 7, 9, 7, 3, 5, 9, 2,\n",
      "        9, 2, 6, 5, 5, 4, 6, 2, 7, 2, 4, 9, 8, 1, 7, 8, 5, 9, 6, 0, 3, 0, 5, 2,\n",
      "        9, 1, 2, 6, 7, 4, 8, 1, 1, 4, 1, 1, 3, 5, 9, 6, 4, 4, 0, 2, 2, 3, 3, 7,\n",
      "        1, 5, 8, 8, 1, 6, 6, 1, 9, 5, 7, 9, 7, 8, 4, 1, 9, 3, 3, 4, 4, 0, 3, 0,\n",
      "        2, 7, 8, 7, 4, 9, 9, 5, 1, 8, 4, 1, 3, 8, 0, 8, 4, 4, 2, 2, 2, 7, 9, 7,\n",
      "        3, 5, 3, 6, 9, 6, 4, 4, 6, 5, 9, 1, 5, 8, 2, 7, 4, 1, 5, 9, 7, 8, 0, 3,\n",
      "        9, 0, 2, 9, 4, 4, 8, 1], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theob\\simclr_doctoral_research\\autoencoder.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research/autoencoder.ipynb#ch0000020?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research/autoencoder.ipynb#ch0000020?line=15'>16</a>\u001b[0m \u001b[39m#show the image\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research/autoencoder.ipynb#ch0000020?line=16'>17</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(img\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mdata[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research/autoencoder.ipynb#ch0000020?line=17'>18</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theob/simclr_doctoral_research/autoencoder.ipynb#ch0000020?line=18'>19</a>\u001b[0m plt\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#show results of the model\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "for data in test_data:\n",
    "    img, label = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img = Variable(img).to(device)\n",
    "    label = Variable(label).to(device)\n",
    "    output = joined_model(img)\n",
    "    pic = to_img(img.cpu().data)\n",
    "    # save_image(pic, './mlp_img/image_test.png')\n",
    "    #show predicted label for the image\n",
    "    print(output.argmax(dim=1))\n",
    "    #show the true label for the image\n",
    "    print(label)\n",
    "    #show the image\n",
    "    plt.imshow(img.cpu().data[0].numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the joined_model\n",
    "torch.save(joined_model.state_dict(), './saved_models/joined_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
